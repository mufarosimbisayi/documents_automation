{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0373ba43-010b-437c-b345-6f763eb9a9ff",
   "metadata": {},
   "source": [
    "**This script keeps track of a signrequest status for particular documents designated by template ID**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d0186-fb66-425d-92e9-dd723d090a14",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e400544d-dc25-4f00-a6d7-a2f521d1d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=1017190226189-f1d5s7cpjrj54u2rqk1ufh9pevguqoap.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import gspread\n",
    "import gspread_dataframe as gd\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "#authenticate Google\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9bcc3-70d5-429d-821d-7a594f780fdb",
   "metadata": {},
   "source": [
    "**Download Documents Functions Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d186bb-5789-49b0-98a2-8c14aa92e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_download_df():\n",
    "    \"\"\"\n",
    "    Imports the student and signrequests dfs and merges them\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "        \n",
    "    Returns:\n",
    "        download_df: A dataframe containing merged signrequests and students data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #import signrequest update sheet\n",
    "    signrequests_df = import_worksheet(\"SRS Cohort 2021 Contracting\", \"Status Updates\")\n",
    "\n",
    "    #import student contract data\n",
    "    student_df = import_worksheet(\"SRS Cohort 2021 Contracting\", \"Applicants\")\n",
    "    \n",
    "    #create dataframe columns\n",
    "    student_columns = [\"First Name\", \"Last Name\", \"Email Address\"]\n",
    "    signrequests_columns = [\"email\", \"contract document\", \"contract document url\", \"code of conduct document\", \"code of conduct document url\"]\n",
    "    download_columns = [\"First Name\", \"Last Name\", \"Email Address\", \"contract document\", \"contract document url\", \"code of conduct document\", \"code of conduct document url\"]\n",
    "\n",
    "    #subset dataframes using columns\n",
    "    student_df = student_df[student_columns]\n",
    "    signrequests_df = signrequests_df[signrequests_columns]\n",
    "\n",
    "    #merge signrequests with students dataframes\n",
    "    download_df = pd.merge(student_df, signrequests_df, how=\"left\", left_on=\"Email Address\", right_on=\"email\")\n",
    "    \n",
    "    return download_df[download_columns]\n",
    "\n",
    "\n",
    "def create_folder_ids(download_df):\n",
    "    \"\"\"\n",
    "    Creates folder ids which represent the folder where the document will be saved.\n",
    "    \n",
    "    Args:\n",
    "        target_df: A dataframe which will contain the folder ids.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    #map to folder id's\n",
    "    download_folder_map = {\"Durban Contract\":\"1R2OKfgQQBbHJHyOTZEwxnaEmfVHUbKEj\", \"CPT Contract\":\"135N7dMe099Xmo3NOCdzEGchgfkbjnfnv\",\n",
    "                         \"JHB Contract\":\"1rfGuq3hJZOmqhSzo57OO9Ar0gI0XICwk\", \"Durban Code of Conduct\":\"11H2eHLbVIVBXkVk5h00BT3fd2uj3LtYJ\",\n",
    "                         \"CPT Code of Conduct\":\"19G_zlCBjkF50uqg_s98p7F5WOuYO7oGE\", \"JHB Code of Conduct\":\"1kekVkH_jYo-GrA624QAcPApp8LN91y5Y\"}\n",
    "    \n",
    "    #create contract file ids\n",
    "    download_df[\"contract_folder_id\"] = download_df[\"contract document\"].map(download_folder_map)\n",
    "\n",
    "    #create code of conduct file ids\n",
    "    download_df[\"code_of_conduct_folder_id\"] = download_df[\"code of conduct document\"].map(download_folder_map)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def create_file_names(download_df):\n",
    "    \"\"\"\n",
    "    Creates file names for the files to be saved.\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe containing the data to create the filenames.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #create contract file names\n",
    "    download_df[\"contract_file_name\"] = download_df[\"First Name\"].str.strip() + \" \" + download_df[\"Last Name\"].str.strip() + \" \" + \"Student-Contract\"\n",
    "\n",
    "    #create code of conduct file names\n",
    "    download_df[\"code_of_conduct_file_name\"] = download_df[\"First Name\"].str.strip() + \" \" + download_df[\"Last Name\"].str.strip() + \" \" + \"Student-Code-of-Conduct\"\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def create_status_columns(download_df):\n",
    "    \"\"\"\n",
    "    Creates status updates columns for the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe for which the status updates columns will be created.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #create contract status column\n",
    "    download_df[\"contract status\"] = np.nan\n",
    "\n",
    "    #create code of conduct status column\n",
    "    download_df[\"code of conduct status\"] = np.nan\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def create_url_columns(download_df):\n",
    "    \"\"\"\n",
    "    Creates status updates columns for the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe for which the status updates columns will be created.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #create contract status column\n",
    "    download_df[\"contract document url\"] = np.nan\n",
    "\n",
    "    #create code of conduct status column\n",
    "    download_df[\"code of conduct document url\"] = np.nan\n",
    "    \n",
    "    return\n",
    "\n",
    "    \n",
    "def subset_download_df(download_df):\n",
    "    \"\"\"\n",
    "    Subsets the download dataframe using set out columns.\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe for which the status updates columns will be created.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #create download columns\n",
    "    download_columns = [\"First Name\", \"Last Name\", \"Email Address\", \"contract_file_name\", \"contract status\", \"contract_folder_id\", \"code_of_conduct_file_name\", \"code of conduct status\", \"code_of_conduct_folder_id\"]\n",
    "\n",
    "    #subset download dataframe using download columns\n",
    "    return download_df[download_columns]\n",
    "    \n",
    "\n",
    "def create_download_df():\n",
    "    \"\"\"\n",
    "    Creates the dataframe containing all the information regarding dowloading files.\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "        \n",
    "    Returns:\n",
    "        download_df: A dataframe containing all the information regarding dowloading files.\n",
    "    \"\"\"\n",
    "    download_df = import_download_df()\n",
    "    create_folder_ids(download_df)\n",
    "    create_file_names(download_df)\n",
    "    create_status_columns(download_df)\n",
    "    download_df = subset_download_df(download_df)\n",
    "\n",
    "    return download_df\n",
    "    \n",
    "    \n",
    "def get_download_df():\n",
    "    \"\"\"\n",
    "    Gets or creates the dataframe containing all data for file downloads.\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "        \n",
    "    Returns:\n",
    "        download_df: A dataframe containing all the information regarding dowloading files.\n",
    "    \"\"\"\n",
    "\n",
    "    #import download update sheet\n",
    "    download_df = import_worksheet(\"SRS Cohort 2021 Contracting\", \"Downloaded Documents\")\n",
    "    if download_df.empty:\n",
    "        download_df = create_download_df()\n",
    "        export_worksheet(\"SRS Cohort 2021 Contracting\", \"Downloaded Documents\", download_df)\n",
    "    create_url_columns(download_df)\n",
    "    return download_df\n",
    "    \n",
    "    \n",
    "def download_file(file_link, file_name):\n",
    "    \"\"\"\n",
    "    Downloads a file from a link.\n",
    "    \n",
    "    Args:\n",
    "        file_link: A string representing the link to the file.\n",
    "        file_name: A string representing the name of the file.\n",
    "        \n",
    "    Returns:\n",
    "        file_path: A string representing a path to the downloaded file.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = requests.get(file_link, allow_redirects=True)\n",
    "    file_path = \"../downloads_folder/\" + file_name + f\".pdf\"\n",
    "    open(file_path, 'wb').write(r.content)\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "\n",
    "def upload_file(file_name, file_path, folder_id):\n",
    "    \"\"\"\n",
    "    Uploads a file to google shared drive folder.\n",
    "    \n",
    "    Args:\n",
    "        file_name: A string representing the name of the file.\n",
    "        file_path: A string representing a path to the file.\n",
    "        folder_id: A string representing the ID of the folder the file is to be uploaded to.\n",
    "        \n",
    "    Returns:\n",
    "        _: None\n",
    "    \"\"\"\n",
    "    \n",
    "    file = drive.CreateFile({\n",
    "        'title': file_name,\n",
    "        'mimeType': 'application/pdf',\n",
    "        'parents': [{\n",
    "            'kind': 'drive',\n",
    "            'teamDriveId': '0ALGjY-PCeStEUk9PVA',\n",
    "            'id': folder_id\n",
    "        }]\n",
    "    })\n",
    "    \n",
    "    file.SetContentFile(file_path)\n",
    "    \n",
    "    file.Upload(param={'supportsTeamDrives': True})\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def delete_local_file(file_path):\n",
    "    \"\"\"\n",
    "    Deletes local file if it exists.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string representing a path to the file.\n",
    "    \n",
    "    Returns:\n",
    "        _: A boolean indicating if a file existed or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    #delete local file\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_link, file_name, folder_id):\n",
    "    \"\"\"\n",
    "    Uploads a file to google drive directly from a link.\n",
    "    \n",
    "    Args:\n",
    "        file_link: A string representing the link to the file.\n",
    "        file_name: A string representing the name of the file.\n",
    "        folder_id: A string representing the ID of the folder the file is to be uploaded to.\n",
    "        \n",
    "    Returns:\n",
    "        _: A boolean indicating if the file upload was a success or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_link:\n",
    "        return False\n",
    "    file_path = download_file(file_link, file_name)\n",
    "    if not file_path:\n",
    "        return False\n",
    "    upload_file(file_name, file_path, folder_id)\n",
    "    if delete_local_file(file_path):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def bulk_upload_to_gdrive(download_df, target_df, upload_map):\n",
    "    \"\"\"\n",
    "    Bulk uploads contracts to google drive.\n",
    "    \n",
    "    Args:\n",
    "    download_df: A dataframe containing all the links to be downloaded and uploaded.\n",
    "    upload_map: A dictionary containing the column names to be used\n",
    "    \n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, row in target_df.iterrows():\n",
    "        \n",
    "        if upload_to_gdrive(row[upload_map[\"document_url\"]], row[upload_map[\"file_name\"]], row[upload_map[\"folder_id\"]]):\n",
    "            download_df.loc[download_df[upload_map[\"file_name\"]] == row[upload_map[\"file_name\"]], upload_map[\"document_status\"]] = 'Saved'\n",
    "        else:\n",
    "            print(f\"The following file could not be uploaded: {row[upload_map['file_name']]}\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dd799-9e28-45ed-b913-9f6d9d0a3308",
   "metadata": {},
   "source": [
    "**Local Functions Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c844982-2917-4e73-a4a3-c5b0c26267ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_worksheet(google_spreadsheet, google_worksheet):\n",
    "    \"\"\"\n",
    "    Imports a google worksheet and saves it as a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        google_spreadsheet: A string representing the target google spreadsheet name.\n",
    "        google_worksheet: A string representing the target google worksheet name.\n",
    "        \n",
    "    Returns:\n",
    "        target_df: A dataframe containing data from the target google worksheet.\n",
    "    \"\"\"\n",
    "    \n",
    "    #authenticate gspread\n",
    "    gc = gspread.oauth()\n",
    "\n",
    "    #import worksheet\n",
    "    data_worksheet = gc.open(google_spreadsheet).worksheet(google_worksheet)\n",
    "    \n",
    "    #import worksheet data to dataframe\n",
    "    target_df = gd.get_as_dataframe(data_worksheet)\n",
    "    \n",
    "    #remove null rows\n",
    "    target_df = target_df.dropna(how='all')\n",
    "    \n",
    "    #remove null columns\n",
    "    new_columns = list(filter(lambda column: True if 'Unnamed' not in column else False, target_df.columns))\n",
    "    target_df = target_df[new_columns]\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "\n",
    "def get_page_metadata(page):\n",
    "    \"\"\"\n",
    "    Gets the SignRequest API metadata for sent signrequests on a single page (10-50 documents)\n",
    "    \n",
    "    Args:\n",
    "        page: An integer representing the page number\n",
    "        \n",
    "    Returns:\n",
    "        response: A requests object containing the SignRequest response.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        \"https://wethinkcode.signrequest.com/api/v1/documents/\",\n",
    "        headers={\"Authorization\": \"Token c37da7fb557f0208fd1fbf18dc6896a5bff4e9ef\"},\n",
    "        params={\"page\":page, \"limit\":20}\n",
    "    )\n",
    "\n",
    "    #print(f\"Get Page Status: {response.status_code}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def check_email(result, emails):\n",
    "    \"\"\"\n",
    "    Checks if result dictionary email value is in the emails list.\n",
    "    \n",
    "    Args:\n",
    "        result: A disctionary containg metadata for a signrequest API response.\n",
    "        emails: A list of strings representing emails.\n",
    "        \n",
    "    Returns:\n",
    "        _: A bool indicating if dictionary email value is in the emails list.\n",
    "    \"\"\"\n",
    "    \n",
    "    return True if result['signrequest']['signers'][-1]['email'] in emails else False\n",
    "\n",
    "\n",
    "def check_template(result, template_ids):\n",
    "    \"\"\"\n",
    "    Checks if result dictionary template id value is the same as template_id\n",
    "    \n",
    "    Args:\n",
    "        result: A disctionary containg metadata for a signrequest API response.\n",
    "        template_id: A string representing the ID of a signrequest template.\n",
    "        \n",
    "    Returns:\n",
    "        _: A bool indicating if result dictionary template id value is the same as template_id\n",
    "    \"\"\"\n",
    "    if not result['template']:\n",
    "        return False\n",
    "    return True if result['template'].split(\"/\")[-2] in template_ids else False\n",
    "\n",
    "\n",
    "def filter_results(results, template_ids, emails):\n",
    "    \"\"\"\n",
    "    Filter API call metadata results by email\n",
    "    \n",
    "    Args:\n",
    "        results: A jsonified API call response.\n",
    "        emails: A list of strings each representing an email.\n",
    "        \n",
    "    Returns:\n",
    "        A filtered list of signrequests metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(\n",
    "        filter(\n",
    "            lambda result: True if check_template(result, template_ids) and check_email(result, emails) else False, results\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def map_status(status_code):\n",
    "    \"\"\"\n",
    "    Maps a status code to the actual status.\n",
    "    \n",
    "    Args:\n",
    "        status_code: A status code representing the status of a signrequest.\n",
    "        \n",
    "    Returns:\n",
    "        _: A string representing the status of a signrequest.\n",
    "    \"\"\"\n",
    "    \n",
    "    signrequest_status_map = {\n",
    "        \"co\":\"converting\", \"ne\":\"new\", \"se\":\"sent\", \"vi\":\"viewed\", \"si\":\"signed\",\n",
    "        \"do\":\"downloaded\", \"sd\":\"signed and downloaded\", \"ca\":\"cancelled\",\n",
    "        \"xp\":\"expired\", \"de\":\"declined\", \"ec\":\"error converting\", \"es\":\"error sending\"\n",
    "    }\n",
    "    return signrequest_status_map[status_code]\n",
    "\n",
    "\n",
    "def create_new_object(result):\n",
    "    \"\"\"\n",
    "    Creates a new object using specific data which is a subset of the old object.\n",
    "    \n",
    "    Args:\n",
    "        result: A json object representing a signrequest result.\n",
    "        \n",
    "    Returns:\n",
    "        new_object: A json object containing subset data of the result object.\n",
    "    \"\"\"\n",
    "    \n",
    "    template_id =  result['template'] if result['template'] else \"Nothing/ \"\n",
    "    new_object = {\n",
    "        \"email\":result['signrequest']['signers'][-1]['email'],\n",
    "        \"template_id\":template_id.split(\"/\")[-2],\n",
    "        \"signrequest_status\":map_status(result[\"status\"]),\n",
    "        \"document_url\":result[\"pdf\"],\n",
    "    }\n",
    "    return new_object\n",
    "\n",
    "\n",
    "def get_specific_data(results_array):\n",
    "    \"\"\"\n",
    "    Gets specific data for each result in the signrequest results array.\n",
    "    \n",
    "    Args:\n",
    "        results_array: An array of objects each representing a signrequest result.\n",
    "        \n",
    "    Returns:\n",
    "        _: An array of objects containing specific data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [create_new_object(result) for result in results_array]\n",
    "\n",
    "\n",
    "def relevant_keys(target_columns):\n",
    "    \"\"\"\n",
    "    Checks that all relevant keys are present.\n",
    "    \n",
    "    Args:\n",
    "        target_columns: All columns of a dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        _: A boolean that indicates that all keys are relevant or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'template_id' not in target_columns:\n",
    "        return False\n",
    "    if 'email' not in target_columns:\n",
    "        return False\n",
    "    if 'signrequest_status' not in target_columns:\n",
    "        return False\n",
    "    if 'document_url' not in target_columns:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def add_download_links(download_df, links_df):\n",
    "    \"\"\"\n",
    "    Adds download links to the download df dataframe.\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe containing all the information regarding dowloading files.\n",
    "        links_df: A dataframe containing links to the documents.\n",
    "        \n",
    "    Returns:\n",
    "        updated_download_df: A dataframe resulting in merging download_df with links_df.\n",
    "    \"\"\"\n",
    "    \n",
    "    #filter links\n",
    "    links_df = links_df[links_df['signrequest_status'].isin([\"signed\", \"downloaded\", \"signed and downloaded\"])]\n",
    "    links_df = links_df.dropna()\n",
    "    \n",
    "    #update download_df\n",
    "    for index, row in links_df.iterrows():\n",
    "        condition_email = download_df['Email Address'] == row['email']\n",
    "        condition_contract = download_df['contract_file_name'].str.contains(row['template_id'], case=False)\n",
    "        condition_conduct = download_df['code_of_conduct_file_name'].str.contains(row['template_id'], case=False)\n",
    "        if download_df[condition_email & condition_contract].shape[0] == 1:\n",
    "            download_df.loc[condition_email & condition_contract, 'contract document url'] = row['document_url']\n",
    "        if download_df[condition_email & condition_conduct].shape[0] == 1:\n",
    "            download_df.loc[condition_email & condition_conduct, 'code of conduct document url'] = row['document_url']\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def upload_documents(download_df, links_df):\n",
    "    \"\"\"\n",
    "    Uploads documents to google drive\n",
    "    \n",
    "    Args:\n",
    "        download_df: A dataframe containing all the document data.\n",
    "        links_df: A dataframe containing links to the documents.\n",
    "        \n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    \n",
    "    campus_document_map = {\"a95c9077-ccdb-48f2-b55f-4e2b5d7f35b9\":\"contract\", \"c28bcf73-98e9-498d-94a2-26f9b7b540b9\":\"conduct\",\n",
    "                      \"280a79c3-d773-466a-8db1-e2a5c5394b60\":\"contract\", \"08c38bcc-0080-40f4-b0b6-4276712887a2\":\"conduct\",\n",
    "                      \"daf83fcc-c68b-4ef0-9ea7-18420732d818\":\"contract\", \"fc1e9cfc-fbd2-4fc6-b852-75d658819670\":\"conduct\"}\n",
    "    \n",
    "    #check all columns are present\n",
    "    if not relevant_keys(links_df.columns):\n",
    "        return\n",
    "    \n",
    "    links_df['template_id'] = links_df['template_id'].map(campus_document_map)\n",
    "    add_download_links(download_df, links_df)\n",
    "    \n",
    "    #set vars for contract\n",
    "    contract_map = {\"document_url\":\"contract document url\", \"file_name\":\"contract_file_name\",\n",
    "                    \"document_status\":\"contract status\", \"folder_id\":\"contract_folder_id\"}\n",
    "    condition_not_null = download_df['contract document url'].notnull()\n",
    "    condition_not_saved = download_df['contract status'] != \"Saved\"\n",
    "    contract_df = download_df[condition_not_null & condition_not_saved]\n",
    "    bulk_upload_to_gdrive(download_df, contract_df, contract_map)\n",
    "    \n",
    "    #set vars for code\n",
    "    code_map = {\"document_url\":\"code of conduct document url\", \"file_name\":\"code_of_conduct_file_name\",\n",
    "                    \"document_status\":\"code of conduct status\", \"folder_id\":\"code_of_conduct_folder_id\"}\n",
    "    condition_not_null = download_df['code of conduct document url'].notnull()\n",
    "    condition_not_saved = download_df['code of conduct status'] != \"Saved\"\n",
    "    conduct_df = download_df[condition_not_null & condition_not_saved]\n",
    "    bulk_upload_to_gdrive(download_df, conduct_df, code_map)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def get_documents_metadata(source_df, download_df):\n",
    "    \"\"\"\n",
    "    Gets API metadata for all signrequests for a particular template.\n",
    "    \n",
    "    Args:\n",
    "        source_df: A dataframe\n",
    "        download_df: A dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        signrequests_list: A list of all relevant SignRequests.\n",
    "    \"\"\"\n",
    "    \n",
    "    #set variables\n",
    "    signrequests_list = []\n",
    "    page_number = 1\n",
    "    \n",
    "    #get unique template ids and emails\n",
    "    emails = get_unique_values(source_df, \"Email Address\")\n",
    "    template_ids = get_unique_values(source_df, [\"Student Contract ID\", \"Code of Conduct ID\"])\n",
    "\n",
    "    #get pages\n",
    "    while True:\n",
    "        response = get_page_metadata(page_number)\n",
    "        json_response = response.json()\n",
    "        filtered_results = filter_results(json_response['results'], template_ids, emails)\n",
    "        specific_filtered_results = get_specific_data(filtered_results)\n",
    "        upload_documents(download_df, pd.DataFrame(specific_filtered_results))\n",
    "        signrequests_list += specific_filtered_results\n",
    "        if json_response['next']:\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return signrequests_list\n",
    "\n",
    "\n",
    "def get_spreadsheet_data():\n",
    "    \"\"\"\n",
    "    Gets and refines the signrequest source data.\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "        \n",
    "    Returns:\n",
    "        source_df: A dataframe containing refined source data used for the signrequests.\n",
    "    \"\"\"\n",
    "    \n",
    "    #import student contract data\n",
    "    student_df = import_worksheet(\"SRS Cohort 2021 Contracting\", \"Applicants\")\n",
    "\n",
    "    #import contract templates id\n",
    "    template_df = import_worksheet(\"SRS Cohort 2021 Contracting\", \"Template IDs\")\n",
    "    \n",
    "    #merge student data and template data\n",
    "    source_df = pd.merge(student_df, template_df, on=\"Campus\")\n",
    "    \n",
    "    #list relevant columns\n",
    "    source_df_columns = [\"Email Address\", \"Student Contract ID\", \"Code of Conduct ID\"]\n",
    "\n",
    "    #subset columns based on list of relevant columns\n",
    "    source_df = source_df[source_df_columns]\n",
    "    \n",
    "    return source_df\n",
    "\n",
    "\n",
    "def get_unique_values(target_df, columns):\n",
    "    \"\"\"\n",
    "    Gets all the unique values in a(the) specified column(s).\n",
    "    \n",
    "    Args:\n",
    "        target_df: A datframe containing the values in one or more columns.\n",
    "        columns: A string or list of strings representing value containing columns.\n",
    "        \n",
    "    Returns:\n",
    "        _: A list of unique values.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_columns = list(filter(lambda column: True if column in columns else False ,target_df.columns))\n",
    "    return pd.unique(target_df[target_columns].values.ravel('K'))\n",
    "\n",
    "\n",
    "def add_signrequest_status_updates(target_df, results_list):\n",
    "    \"\"\"\n",
    "    Adds status updates to the target df based on template id values.\n",
    "    \n",
    "    Args:\n",
    "        target_df: The dataframe to which updates are added.\n",
    "        template_columns: The columns containing the template ids.\n",
    "        updates: A list of objects containing signrequests data.\n",
    "        \n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    contracts = [\"a95c9077-ccdb-48f2-b55f-4e2b5d7f35b9\", \"280a79c3-d773-466a-8db1-e2a5c5394b60\", \"daf83fcc-c68b-4ef0-9ea7-18420732d818\"]\n",
    "    code_of_conduct = [\"c28bcf73-98e9-498d-94a2-26f9b7b540b9\", \"08c38bcc-0080-40f4-b0b6-4276712887a2\", \"fc1e9cfc-fbd2-4fc6-b852-75d658819670\"]\n",
    "    results_df[\"contracts\"] = [True if template_id in contracts else False for template_id in results_df[\"template_id\"]]\n",
    "    results_df[\"code_of_conduct\"] = [True if template_id in code_of_conduct else False for template_id in results_df[\"template_id\"]]\n",
    "    contracts_df = results_df[results_df[\"contracts\"]]\n",
    "    code_of_conduct_df = results_df[results_df[\"code_of_conduct\"]]\n",
    "    results_columns = [\"email\", \"template_id\", \"signrequest_status\", \"document_url\"]\n",
    "    contracts_columns = [\"email\", \"template_id\", \"contracts status\", \"contracts document_url\"]\n",
    "    code_of_conduct_columns = [\"email\", \"template_id\", \"code of conduct status\", \"code of conduct document_url\"]\n",
    "    contracts_df = contracts_df[results_columns]\n",
    "    contracts_df.columns = contracts_columns\n",
    "    code_of_conduct_df = code_of_conduct_df[results_columns]\n",
    "    code_of_conduct_df.columns = code_of_conduct_columns\n",
    "    new_results_df = pd.merge(contracts_df, code_of_conduct_df, on=[\"email\"], how=\"left\")\n",
    "    campus_document_map = {\"a95c9077-ccdb-48f2-b55f-4e2b5d7f35b9\":\"CPT Contract\", \"c28bcf73-98e9-498d-94a2-26f9b7b540b9\":\"CPT Code of Conduct\",\n",
    "                      \"280a79c3-d773-466a-8db1-e2a5c5394b60\":\"Durban Contract\", \"08c38bcc-0080-40f4-b0b6-4276712887a2\":\"Durban Code of Conduct\",\n",
    "                      \"daf83fcc-c68b-4ef0-9ea7-18420732d818\":\"JHB Contract\", \"fc1e9cfc-fbd2-4fc6-b852-75d658819670\":\"JHB Code of Conduct\"}\n",
    "    new_results_df[\"template_id_x\"] = new_results_df[\"template_id_x\"].map(campus_document_map)\n",
    "    new_results_df[\"template_id_y\"] = new_results_df[\"template_id_y\"].map(campus_document_map)\n",
    "    new_columns = [\"email\", \"contract document\", \"contract status\", \"code of conduct document\", \"code of conduct status\", \"contract document url\" ,\"code of conduct document url\"]\n",
    "    old_columns = [\"email\", \"contract document\", \"contract status\", \"contract document url\", \"code of conduct document\", \"code of conduct status\" ,\"code of conduct document url\"]\n",
    "    new_results_df.columns = old_columns\n",
    "    \n",
    "    return new_results_df[new_columns]\n",
    "\n",
    "\n",
    "def export_worksheet(google_spreadsheet, google_worksheet, target_df):\n",
    "    \"\"\"\n",
    "    Exports data from a dataframe onto a google worksheet on a particular spreadsheet.\n",
    "    \n",
    "    Args:\n",
    "        google_spreadsheet: A string representing the target google spreadsheet name.\n",
    "        google_worksheet: A string representing the target google worksheet name.\n",
    "        target_df: A dataframe containing data to load ont the target google worksheet.\n",
    "    \n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    \n",
    "    #authenticate gspread\n",
    "    gc = gspread.oauth()\n",
    "\n",
    "    #import worksheet\n",
    "    data_worksheet = gc.open(google_spreadsheet).worksheet(google_worksheet)\n",
    "    \n",
    "    #export cohort file ids\n",
    "    gd.set_with_dataframe(data_worksheet, target_df)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f2821-b6c9-4561-af81-d37e8b140834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2ee1b-926d-4621-827a-028d68a5cae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be027162-c731-4588-a83a-cfe3da5730d1",
   "metadata": {},
   "source": [
    "**Execute Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72494374-588e-4530-a909-d1b69b1ccd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get signrequests source data\n",
    "source_df = get_spreadsheet_data()\n",
    "\n",
    "#get download dataframe\n",
    "download_df = get_download_df()\n",
    "\n",
    "#get signrequests metadata\n",
    "results_list = get_documents_metadata(source_df, download_df)\n",
    "\n",
    "#add signrequest status updates to source dataframe\n",
    "updated_df = add_signrequest_status_updates(source_df, results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d7f532b-f810-4f50-a3a3-8b37378cfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export updated signrequest source dataframe to google worksheet\n",
    "export_worksheet(\"SRS Cohort 2021 Contracting\" ,\"Status Updates\", updated_df)\n",
    "export_worksheet(\"SRS Cohort 2021 Contracting\", \"Downloaded Documents\", download_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662eb6ef-80b4-4fd5-80a6-abc3dc150afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef89204-03dd-4058-8942-48664320c06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71887ca4-0d68-4ce5-a7ac-b5fb550e1732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a03502a0-58cc-4f9b-b072-bb1074b511d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3eaa29ce-6e0f-4362-984f-b5b1f4c1a534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a38fd419-f90a-4d7b-8c22-79d8aca282a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d099c-cbed-424b-a27c-0df8572679be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6e5e5-8445-4f99-ba16-dd7ef1242938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "26f4e898-ddac-4679-8fbf-b7e44e91b9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "fbf72a8d-bf24-42d8-9b95-ee4635d1cf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5d96a7b4-480f-4fe1-b481-19bef702bfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e6ca416f-49be-473d-8911-f1aff5b1f1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ab66c29c-82f0-4c3a-9474-bd333c1d1f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "87513a5c-a93e-43f4-98c7-ed982271a4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3a49f8a1-7540-4dd9-8944-00f15e6bb06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ee691900-68a3-4213-ba5c-80c3aa898178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222b345-410c-4321-a126-bd00637e13b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dae76-19b5-4913-8cc6-6f1e4caa8200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e1782-b236-478c-ab3b-89cfb94fe791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
